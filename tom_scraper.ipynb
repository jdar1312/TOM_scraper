{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOM SCRAPER\n",
    "Author: Jude Darmanin\n",
    "Date: December 2023\n",
    "\n",
    "This code scrapes all articles from timesofmalta.com from the chosen category and stores them in an SQL database. Was written as part of my Univerity of London degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json \n",
    "from time import sleep\n",
    "import itertools\n",
    "from datetime import datetime as dt\n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET VARIABLES\n",
    "header = {'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'}\n",
    "\n",
    "category = 'national' #select which category to scrape\n",
    "\n",
    "f_url = 'tom_urls.txt'\n",
    "\n",
    "db_TOM = 'articles_TOM.db'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SCRAPE ARTICLE URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape all URLs\n",
    "urls = []\n",
    "\n",
    "for i in itertools.count(start=1):\n",
    "    try:\n",
    "        html = requests.get(f'https://timesofmalta.com/articles/listing/{category}/page:{i}', headers = header).content\n",
    "        soup = BeautifulSoup(html,\"html.parser\")\n",
    "        section = soup.find('script', id = 'listing-ld')\n",
    "        listings = json.loads(section.contents[0])['@graph'] #script data is in json format; 'contents' returns content within HTML tags. @graph object contains listing metadata.\n",
    "        for l in listings:\n",
    "            url = l['url']\n",
    "            urls.append(url)\n",
    "        sleep(0.1)\n",
    "    except:\n",
    "        print(f'Script stopped at page {i}')\n",
    "        break\n",
    "    \n",
    "#save to txt file\n",
    "with open(f_url, 'a') as f:\n",
    "    for u in urls:\n",
    "        f.write(f\"{u}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SCRAPE AND STORE ARTICLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE TABLE/DB to store scraped data\n",
    "sql_create_table = \"\"\"CREATE TABLE IF NOT EXISTS articles (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    title TEXT,\n",
    "                    date DATE,\n",
    "                    article TEXT,\n",
    "                    keywords TEXT,\n",
    "                    author TEXT,\n",
    "                    publisher TEXT,\n",
    "                    url TEXT,\n",
    "                    main_img_url TEXT,\n",
    "                    main_img_cap TEXT);\"\"\"\n",
    "\n",
    "\n",
    "conn = sqlite3.connect(db_TOM)\n",
    "cursor = conn.cursor()\n",
    "# Create a table to store your articles\n",
    "cursor.execute(sql_create_table)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "def store_data(db, data:dict):\n",
    "    '''Function to store data in db'''\n",
    "\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        INSERT INTO articles (title, date, article, keywords, author, publisher, url, main_img_url, main_img_cap)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    ''', (data['title'], data['date'], data['article'], data['keywords'], data['author'], data['publisher'], data['url'], data['main_img_url'], data['main_img_cap']))\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCRAPING ARTICLES \n",
    "\n",
    "#read urls\n",
    "with open(f_url, 'r') as f:\n",
    "    rurls = f.readlines()\n",
    "\n",
    "#scraper\n",
    "errors = []\n",
    "for counter, u in enumerate(rurls): \n",
    "\n",
    "    try:\n",
    "        html=BeautifulSoup(requests.get(u).content,\"html.parser\")\n",
    "        script = html.find('script',{'id':'article-ld'}) \n",
    "        content = json.loads(script.contents[0])['@graph'][0]\n",
    "        \n",
    "        data = {}\n",
    "\n",
    "        data['title'] = content['headline']\n",
    "        data['publisher'] = content['publisher']['@id']\n",
    "        data['keywords'] = content['keywords']\n",
    "        data['date'] = dt.strptime(content['datePublished'], '%Y-%m-%dT%H:%M:%S%z').strftime('%Y-%m-%d')\n",
    "        data['author'] = content['author'][0]['name']\n",
    "        data['article'] = content['articleBody']\n",
    "        data['main_img_url'] = content['image'][0]['url']\n",
    "        data['main_img_cap'] = content['image'][0]['caption']\n",
    "        data['url'] = u\n",
    "\n",
    "        store_data(db_TOM, data)\n",
    "    except:\n",
    "        errors.append(u)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### READING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_names(db, table_name):\n",
    "    '''Get table column names'''\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    #use PRAGMA to get column names\n",
    "    cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "    columns_info = cursor.fetchall()\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    #extract column names \n",
    "    column_names = [column_info[1] for column_info in columns_info]\n",
    "\n",
    "    return column_names\n",
    "\n",
    "\n",
    "def get_all_articles(db):\n",
    "    '''Read articles from DB'''\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute('SELECT * FROM articles')\n",
    "    articles = cursor.fetchall()\n",
    "\n",
    "    column_names = get_column_names(db, 'articles')\n",
    "    articles = [dict(zip(column_names, article)) for article in articles]\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    return articles\n",
    "\n",
    "#reading\n",
    "all_articles = get_all_articles(db_TOM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
